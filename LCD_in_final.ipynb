{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9fac3a1d",
      "metadata": {
        "id": "9fac3a1d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387b097a",
      "metadata": {
        "id": "387b097a"
      },
      "outputs": [],
      "source": [
        "class SentenceEncoder(nn.Module):\n",
        "    def __init__(self, model_name=\"SI2M-Lab/DarijaBERT\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model=AutoModel.from_pretrained(model_name)\n",
        "        self.output_dim=self.model.config.hidden_size\n",
        "        #Freeze all layers by default\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        #Unfreeze the top 4 layers\n",
        "        for i in range(8, 12):\n",
        "            for param in self.model.encoder.layer[i].parameters():\n",
        "                param.requires_grad=True\n",
        "\n",
        "    def forward(self, sentences: list[str]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input: list of utterances\n",
        "        Output: Tensor of shape (len(sentences), hidden_size)\n",
        "        \"\"\"\n",
        "        encoded=self.tokenizer(sentences, padding=True, max_length=128,truncation=True, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "\n",
        "        output=self.model(**encoded)\n",
        "\n",
        "        # Mean pooling over token embeddings\n",
        "\n",
        "        embeddings=output.last_hidden_state\n",
        "        attention_mask=encoded[\"attention_mask\"].unsqueeze(-1)\n",
        "        masked_embeddings=embeddings*attention_mask\n",
        "        summed=masked_embeddings.sum(dim=1)\n",
        "        counts=attention_mask.sum(dim=1).clamp(min=1)\n",
        "\n",
        "        return summed/counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883cddc7",
      "metadata": {
        "id": "883cddc7"
      },
      "outputs": [],
      "source": [
        "def build_pairwise_features(s:torch.Tensor, t:torch.Tensor) ->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Given two senetence embeddings s and t of shape [d]\n",
        "    the function returns feature vector of shape [5*d]: [s,t,s-t,|s-t|,s*t]\n",
        "    \"\"\"\n",
        "    diff=s-t\n",
        "    abs_diff=torch.abs(s-t)\n",
        "    dot=s*t\n",
        "\n",
        "    features=torch.cat([s,t,diff, abs_diff, dot], dim=-1)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22bd1b0d",
      "metadata": {
        "id": "22bd1b0d"
      },
      "outputs": [],
      "source": [
        "class LCD_scorer(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=128, hidden_dropout=0.1, input_dropout=0.1):\n",
        "        super().__init__()\n",
        "        input_dim=5*embedding_dim\n",
        "        #initializing the one layer MLP\n",
        "        self.mlp=nn.Sequential(\n",
        "            nn.Dropout(input_dropout),\n",
        "            nn.Linear(input_dim,hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(hidden_dropout),\n",
        "            nn.Linear(hidden_dim,1) #to output a single score\n",
        "        )\n",
        "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        return self.mlp(features).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41929050",
      "metadata": {
        "id": "41929050"
      },
      "outputs": [],
      "source": [
        "def load_dialogues_with_speakers(folder_path=\"/content/final_datasets\"):\n",
        "    all_dialogues = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jsonl\"):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        raw_dialogue = data.get(\"dialogue\", [])\n",
        "\n",
        "                        # Extract list of (speaker, utterance)\n",
        "                        dialogue = []\n",
        "                        for turn in raw_dialogue:\n",
        "                            if isinstance(turn, dict):\n",
        "                                for speaker, utterance in turn.items():\n",
        "                                    dialogue.append((speaker, utterance))\n",
        "                        if len(dialogue) >= 2:  # skip too-short dialogues\n",
        "                            all_dialogues.append(dialogue)\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"Warning: could not parse line in {filename}\")\n",
        "\n",
        "    return all_dialogues # Each dialogue has form: [(speaker, utterance), ...], all_dialogues of form [dialogue1, dialogue2, ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YfBIottqmZ3F",
      "metadata": {
        "id": "YfBIottqmZ3F"
      },
      "outputs": [],
      "source": [
        "def bounded_margin_loss(\n",
        "    score_pos,\n",
        "    score_neg,\n",
        "    margin=10,\n",
        "    upper_bound=10.0,\n",
        "    lower_bound=0.0,\n",
        "    lambda_upper=1.0,\n",
        "    lambda_lower=1.0\n",
        "):\n",
        "    # standard margin ranking loss\n",
        "    margin_loss = torch.clamp(margin - (score_pos - score_neg), min=0)\n",
        "    # Upper and lower bound penalties\n",
        "    upper_penalty = torch.clamp(score_pos - upper_bound, min=0)\n",
        "    lower_penalty = torch.clamp(lower_bound - score_pos, min=0)\n",
        "    loss = margin_loss + lambda_upper * upper_penalty + lambda_lower * lower_penalty\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b830b8",
      "metadata": {
        "id": "64b830b8"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(dialogues, encoder, scorer_fwd, scorer_bwd, criterion, optimizer_fwd, optimizer_bwd, device):\n",
        "    scorer_fwd.train()\n",
        "    scorer_bwd.train()\n",
        "    encoder.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for dialogue in dialogues:\n",
        "        if len(dialogue) < 3:\n",
        "            continue\n",
        "        speakers, utterances = zip(*dialogue)\n",
        "        #using the sentence encoder, get the embeddings of the utterances\n",
        "        senetence_embeddings = encoder(list(utterances)).to(device)\n",
        "        dialogue_loss = 0\n",
        "        pair_count = 0\n",
        "\n",
        "        for i in range(len(senetence_embeddings) - 1):\n",
        "            s_i = senetence_embeddings[i].unsqueeze(0)\n",
        "            s_pos = senetence_embeddings[i + 1].unsqueeze(0)\n",
        "            current_speaker = speakers[i]\n",
        "\n",
        "            # Sample negatives from the opposite speaker (not i+1)\n",
        "            candidates = [\n",
        "                j for j in range(len(senetence_embeddings))\n",
        "                if j != i + 1 and speakers[j] != current_speaker\n",
        "            ]\n",
        "            if not candidates:\n",
        "                continue\n",
        "            neg_idx = random.choice(candidates)\n",
        "            s_neg = senetence_embeddings[neg_idx].unsqueeze(0)\n",
        "\n",
        "            # Build features for both directions\n",
        "            feat_pos_fwd = build_pairwise_features(s_i, s_pos).to(device)\n",
        "            feat_neg_fwd = build_pairwise_features(s_i, s_neg).to(device)\n",
        "            feat_pos_bwd = build_pairwise_features(s_pos, s_i).to(device)\n",
        "            feat_neg_bwd = build_pairwise_features(s_neg, s_i).to(device)\n",
        "\n",
        "            # Score the features\n",
        "            score_pos_fwd = scorer_fwd(feat_pos_fwd.unsqueeze(0)).view(1)\n",
        "            score_neg_fwd = scorer_fwd(feat_neg_fwd.unsqueeze(0)).view(1)\n",
        "            score_pos_bwd = scorer_bwd(feat_pos_bwd.unsqueeze(0)).view(1)\n",
        "            score_neg_bwd = scorer_bwd(feat_neg_bwd.unsqueeze(0)).view(1)\n",
        "\n",
        "            # Average scores\n",
        "            score_pos = (score_pos_fwd + score_pos_bwd) / 2\n",
        "            score_neg = (score_neg_fwd + score_neg_bwd) / 2\n",
        "\n",
        "            # Compute loss\n",
        "            loss = bounded_margin_loss(\n",
        "                score_pos,\n",
        "                score_neg,\n",
        "            )\n",
        "\n",
        "            dialogue_loss += loss\n",
        "            pair_count += 1\n",
        "        if pair_count > 0:\n",
        "            # Backpropagate and update\n",
        "            dialogue_loss = dialogue_loss / pair_count\n",
        "            optimizer_fwd.zero_grad()\n",
        "            optimizer_bwd.zero_grad()\n",
        "            dialogue_loss.backward()\n",
        "            optimizer_fwd.step()\n",
        "            optimizer_bwd.step()\n",
        "\n",
        "            total_loss += dialogue_loss.item()\n",
        "            #Increment the correct counter if the score of the positive is bigger than the negative one\n",
        "            if score_pos.detach().item() > score_neg.detach().item():\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "\n",
        "    return total_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c342a5",
      "metadata": {
        "id": "72c342a5"
      },
      "outputs": [],
      "source": [
        "def eval_one_epoch(dialogues, encoder, scorer_fwd, scorer_bwd, criterion, device):\n",
        "    scorer_fwd.eval()\n",
        "    scorer_bwd.eval()\n",
        "    encoder.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dialogue in dialogues:\n",
        "            if len(dialogue) < 3:\n",
        "                continue\n",
        "            speakers, utterances = zip(*dialogue)\n",
        "            senetence_embeddings = encoder(list(utterances)).to(device)\n",
        "\n",
        "            for i in range(len(senetence_embeddings) - 1):\n",
        "                s_i = senetence_embeddings[i].unsqueeze(0).detach()\n",
        "                s_pos = senetence_embeddings[i + 1].unsqueeze(0).detach()\n",
        "                current_speaker = speakers[i]\n",
        "\n",
        "                # Sample negatives from the same speaker (not i+1)\n",
        "                candidates = [\n",
        "                    j for j in range(len(senetence_embeddings))\n",
        "                    if j != i + 1 and speakers[j] != current_speaker\n",
        "                ]\n",
        "                if not candidates:\n",
        "                    continue\n",
        "                neg_idx = random.choice(candidates)\n",
        "                s_neg = senetence_embeddings[neg_idx].unsqueeze(0).detach()\n",
        "\n",
        "                # Build features for both directions\n",
        "                feat_pos_fwd = build_pairwise_features(s_i, s_pos).to(device)\n",
        "                feat_neg_fwd = build_pairwise_features(s_i, s_neg).to(device)\n",
        "                feat_pos_bwd = build_pairwise_features(s_pos, s_i).to(device)\n",
        "                feat_neg_bwd = build_pairwise_features(s_neg, s_i).to(device)\n",
        "\n",
        "                # Score the features\n",
        "                score_pos_fwd = scorer_fwd(feat_pos_fwd.unsqueeze(0)).view(1)\n",
        "                score_neg_fwd = scorer_fwd(feat_neg_fwd.unsqueeze(0)).view(1)\n",
        "                score_pos_bwd = scorer_bwd(feat_pos_bwd.unsqueeze(0)).view(1)\n",
        "                score_neg_bwd = scorer_bwd(feat_neg_bwd.unsqueeze(0)).view(1)\n",
        "\n",
        "                # Average scores\n",
        "                score_pos = (score_pos_fwd + score_pos_bwd) / 2\n",
        "                score_neg = (score_neg_fwd + score_neg_bwd) / 2\n",
        "                # Compute loss\n",
        "                target = torch.tensor([1.0], device=device)\n",
        "\n",
        "                loss = bounded_margin_loss(\n",
        "                    score_pos,\n",
        "                    score_neg,\n",
        "                )\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if score_pos.detach().item() > score_neg.detach().item():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "    return total_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77da0776",
      "metadata": {
        "id": "77da0776"
      },
      "outputs": [],
      "source": [
        "def train_val_split():\n",
        "    all_dialogues=load_dialogues_with_speakers()\n",
        "    train_d, test_val_d= train_test_split(all_dialogues, test_size=0.15, shuffle=True)\n",
        "    test_d, val_d=train_test_split(test_val_d, test_size=0.333, shuffle=True)\n",
        "    return train_d, test_d ,val_d #train (85%) test(10%) val(5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MFAR-Re9AIt8",
      "metadata": {
        "id": "MFAR-Re9AIt8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def permute_dialogue(dialogue, speaker_to_permute, num_permutations=1):\n",
        "    # Gather indices and utterances of the target speaker\n",
        "    speaker_indices = []\n",
        "    speaker_utterances = []\n",
        "    other_turns = []\n",
        "\n",
        "    for idx, turn in enumerate(dialogue):\n",
        "        if turn[0] == speaker_to_permute:\n",
        "            speaker_indices.append(idx)\n",
        "            speaker_utterances.append(turn[1])\n",
        "        else:\n",
        "            other_turns.append((idx, turn))\n",
        "\n",
        "    permutations = []\n",
        "    for _ in range(num_permutations):\n",
        "        if len(speaker_indices) < 2:\n",
        "            # Not enough to permute, just return original dialogue\n",
        "            permutations.append(dialogue.copy())\n",
        "            continue\n",
        "\n",
        "        # Randomly choose how many utterances to permute (at least 2)\n",
        "        k = random.randint(2, len(speaker_indices))\n",
        "        # Pick random indices to permute\n",
        "        permute_sub_idx = random.sample(range(len(speaker_indices)), k)\n",
        "        # Extract their utterances and shuffle\n",
        "        to_shuffle = [speaker_utterances[i] for i in permute_sub_idx]\n",
        "        shuffled = to_shuffle.copy()\n",
        "        while True:\n",
        "            random.shuffle(shuffled)\n",
        "            if shuffled != to_shuffle:\n",
        "                break\n",
        "\n",
        "        # Build a new utterance list for the target speaker\n",
        "        new_speaker_utterances = speaker_utterances.copy()\n",
        "        for orig, new in zip(permute_sub_idx, shuffled):\n",
        "            new_speaker_utterances[orig] = new\n",
        "\n",
        "        # Reconstruct dialogue\n",
        "        permuted = []\n",
        "        speaker_ptr = 0\n",
        "        for idx, (speaker, _) in enumerate(dialogue):\n",
        "            if speaker == speaker_to_permute:\n",
        "                permuted.append((speaker, new_speaker_utterances[speaker_ptr]))\n",
        "                speaker_ptr += 1\n",
        "            else:\n",
        "                # Get original non-permuted turn at this index\n",
        "                orig_idx, orig_turn = other_turns.pop(0)\n",
        "                permuted.append(orig_turn)\n",
        "\n",
        "        permutations.append(permuted)\n",
        "\n",
        "    return permutations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bIvWH7estsqo",
      "metadata": {
        "id": "bIvWH7estsqo"
      },
      "outputs": [],
      "source": [
        "def compute_dialogue_pred(dialogue, encoder, scorer_fwd, scorer_bwd, device):\n",
        "    speakers, utterances = zip(*dialogue)\n",
        "    embeddings = encoder(list(utterances)).to(device)\n",
        "\n",
        "    score_sum = 0\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        s_i = embeddings[i].to(device)\n",
        "        s_j = embeddings[i + 1].to(device)\n",
        "\n",
        "        feat_fwd = build_pairwise_features(s_i, s_j).to(device)\n",
        "        feat_bwd = build_pairwise_features(s_j, s_i).to(device)\n",
        "\n",
        "        score_fwd = scorer_fwd(feat_fwd.unsqueeze(0)).view(1)\n",
        "        score_bwd = scorer_bwd(feat_bwd.unsqueeze(0)).view(1)\n",
        "\n",
        "        score = (score_fwd + score_bwd) / 2\n",
        "\n",
        "        if score <0:\n",
        "          return 0 # return 0 (as incoherent) if the score of a pair is negative \n",
        "\n",
        "\n",
        "    return 1 # return 1 (as coherent) if no pair has a negative score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cbfd782",
      "metadata": {
        "id": "4cbfd782"
      },
      "outputs": [],
      "source": [
        "def test_experiment(dialogues, encoder, scorer_fwd, scorer_bwd, device , num_permutations=1):\n",
        "    encoder.eval()\n",
        "    scorer_fwd.eval()\n",
        "    scorer_bwd.eval()\n",
        "\n",
        "    true=[]\n",
        "    pred=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dialogue in dialogues:\n",
        "            if len(dialogue)< 3:\n",
        "                continue\n",
        "            #predict the class(coherent 1 or incoherent 0) of the original dialogue\n",
        "            score=compute_dialogue_pred(dialogue,encoder, scorer_fwd, scorer_bwd, device)\n",
        "            true.append(1)\n",
        "            pred.append(score)\n",
        "\n",
        "            #predict the class of the permuted dialogues in A's or B's utterances\n",
        "            choice = random.choice([\"A\", \"B\"])\n",
        "            permuted=permute_dialogue(dialogue, choice, num_permutations=num_permutations)\n",
        "            for perm in permuted:\n",
        "                score2=compute_dialogue_pred(perm, encoder, scorer_fwd, scorer_bwd, device)\n",
        "                true.append(0)\n",
        "                pred.append(score2)\n",
        "\n",
        "            \n",
        "        f1=f1_score(true , pred)\n",
        "        accuracy=accuracy_score(true, pred)\n",
        "\n",
        "        return f1, accuracy,  true, pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5567adb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5567adb5",
        "outputId": "1fe87de6-575e-4970-ff77-f284862945e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: could not parse line in clothing_dialogues.jsonl\n",
            "Warning: could not parse line in clothing_dialogues.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/8: 100%|██████████ [elapsed: 00:40]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4876.0948, Train Acc: 0.7790, Val Loss: 1851.9860, Val Acc: 0.7016\n",
            "Saving the current best model with validation accuracy 0.7016129032258065 at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/8: 100%|██████████ [elapsed: 00:38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3570.0419, Train Acc: 0.7935, Val Loss: 1534.3780, Val Acc: 0.7298\n",
            "Saving the current best model with validation accuracy 0.7298387096774194 at epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/8: 100%|██████████ [elapsed: 00:38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2989.9386, Train Acc: 0.8007, Val Loss: 1421.7869, Val Acc: 0.7782\n",
            "Saving the current best model with validation accuracy 0.7782258064516129 at epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/8: 100%|██████████ [elapsed: 00:37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2580.6839, Train Acc: 0.8225, Val Loss: 1491.6307, Val Acc: 0.7863\n",
            "Saving the current best model with validation accuracy 0.7862903225806451 at epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/8: 100%|██████████ [elapsed: 00:38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2342.1787, Train Acc: 0.8478, Val Loss: 1251.9556, Val Acc: 0.8145\n",
            "Saving the current best model with validation accuracy 0.8145161290322581 at epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/8: 100%|██████████ [elapsed: 00:38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2082.8186, Train Acc: 0.8786, Val Loss: 1245.5516, Val Acc: 0.7782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/8: 100%|██████████ [elapsed: 00:37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1835.9685, Train Acc: 0.9058, Val Loss: 1266.0744, Val Acc: 0.7984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/8: 100%|██████████ [elapsed: 00:38]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1665.2188, Train Acc: 0.9203, Val Loss: 1307.8588, Val Acc: 0.8145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__==\"__main__\":\n",
        "    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoder=SentenceEncoder()\n",
        "    encoder.to(device)\n",
        "    scorer_fwd=LCD_scorer()\n",
        "    scorer_bwd=LCD_scorer()\n",
        "    scorer_fwd.to(device)\n",
        "    scorer_bwd.to(device)\n",
        "    criterion=nn.MarginRankingLoss(margin=10)\n",
        "    params_fwd = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(scorer_fwd.parameters())\n",
        "    params_bwd = list(filter(lambda p: p.requires_grad, encoder.parameters())) + list(scorer_bwd.parameters())\n",
        "    optimizer_fwd=torch.optim.Adam(params_fwd, lr=1e-5)\n",
        "    optimizer_bwd=torch.optim.Adam(params_bwd, lr=1e-5)\n",
        "\n",
        "\n",
        "    train_data, test_data ,val_data=train_val_split()\n",
        "\n",
        "    num_epochs=8\n",
        "\n",
        "    best_val_acc=0.0\n",
        "\n",
        "    # Training loop, saving the model whenever we reach a higher validation accuracy to avoid overfitting \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        with tqdm(total=1, desc=f\"Epoch {epoch+1}/{num_epochs}\", bar_format=\"{l_bar}{bar} [elapsed: {elapsed}]\") as pbar:\n",
        "            train_loss, train_acc = train_one_epoch(dialogues=train_data, encoder=encoder, scorer_fwd=scorer_fwd, scorer_bwd=scorer_bwd, criterion=criterion, optimizer_fwd=optimizer_fwd, optimizer_bwd=optimizer_bwd, device=device)\n",
        "            val_loss, val_acc = eval_one_epoch(dialogues=val_data, encoder=encoder, scorer_fwd=scorer_fwd, scorer_bwd=scorer_bwd, criterion=criterion, device=device)\n",
        "\n",
        "\n",
        "            pbar.update(1)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        if val_acc>best_val_acc:\n",
        "            best_val_acc=val_acc\n",
        "            print(f\"Saving the current best model with validation accuracy {val_acc} at epoch {epoch+1}\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"encoder_state_dict\": encoder.state_dict(),\n",
        "                    \"scorer_fwd_state_dict\": scorer_fwd.state_dict(),\n",
        "                    \"scorer_bwd_state_dict\": scorer_bwd.state_dict()\n",
        "                }, \"/content/best_lcd_in.pt\"\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jIJebXtX-CLw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIJebXtX-CLw",
        "outputId": "36b6c540-c4cd-478e-d032-b726f9cd0026"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Test: 100%|██████████ [elapsed: 00:01]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7627118644067796\n",
            "accuracy score: 0.7846153846153846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#load the model wiht the best validation accuracy and testing it on binary classification \n",
        "\n",
        "checkpoint = torch.load(\"/content/best_lcd_in.pt\", map_location=\"cuda\")\n",
        "encoder=SentenceEncoder()\n",
        "encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "encoder.to(device)\n",
        "scorer_fwd=LCD_scorer()\n",
        "scorer_fwd.load_state_dict(checkpoint[\"scorer_fwd_state_dict\"])\n",
        "scorer_bwd=LCD_scorer()\n",
        "scorer_bwd.load_state_dict(checkpoint[\"scorer_bwd_state_dict\"])\n",
        "scorer_fwd.to(device)\n",
        "scorer_bwd.to(device)\n",
        "\n",
        "with tqdm(total=1, desc=f\"Test\", bar_format=\"{l_bar}{bar} [elapsed: {elapsed}]\") as pbar:\n",
        "    f1,accuracy, true, pred=test_experiment(test_data, encoder, scorer_fwd, scorer_bwd, 0, device)\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "print(f\"F1 score: {f1}\")\n",
        "print(f\"accuracy score: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDuwlNnZChY1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDuwlNnZChY1",
        "outputId": "5ca5b66d-7436-4176-f0f3-6dc27a1ceefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[57  8]\n",
            " [20 45]]\n"
          ]
        }
      ],
      "source": [
        "#Printing the confusion matrix \n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(true, pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
