{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jcS29gdFnUVf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tew7dXUhDP8_"
   },
   "outputs": [],
   "source": [
    "class DialogueCoherenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    For each dialogue, adds:\n",
    "        - The original (label=1)\n",
    "        - One negative: shuffle a random subset (between 2 and all) of one speaker's utterances, guaranteed different from original if possible.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder, max_utterances=16, seed=42):\n",
    "        self.dialogues = []\n",
    "        self.labels = []\n",
    "        self.max_utterances = max_utterances\n",
    "        random.seed(seed)\n",
    "\n",
    "        all_dialogues = []\n",
    "\n",
    "        for file in glob(os.path.join(data_folder, \"*.jsonl\")):\n",
    "            with open(file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        raw_dialogue = obj.get(\"dialogue\", [])\n",
    "                        dialogue = []\n",
    "                        for turn in raw_dialogue:\n",
    "                            if isinstance(turn, dict):\n",
    "                                for speaker, utterance in turn.items():\n",
    "                                    dialogue.append((speaker, utterance))\n",
    "                        if len(dialogue) >= 2:\n",
    "                            dialogue = dialogue[:self.max_utterances]\n",
    "                            all_dialogues.append(dialogue)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: could not parse line in {file}\")\n",
    "\n",
    "        if not all_dialogues:\n",
    "            raise ValueError(\"No valid dialogues found in the provided folder.\")\n",
    "\n",
    "        for original_dialogue in all_dialogues:\n",
    "            self.dialogues.append(original_dialogue)\n",
    "            self.labels.append(1)\n",
    "\n",
    "            speakers = list({spk for spk, _ in original_dialogue})\n",
    "            if not speakers or len(speakers) < 2:\n",
    "                continue\n",
    "\n",
    "            valid_speakers = [spk for spk in speakers if sum(1 for s, _ in original_dialogue if s == spk) > 1]\n",
    "            if not valid_speakers:\n",
    "                continue\n",
    "            target_spk = random.choice(valid_speakers)\n",
    "            indices = [idx for idx, (spk, _) in enumerate(original_dialogue) if spk == target_spk]\n",
    "            utts = [original_dialogue[idx][1] for idx in indices]\n",
    "\n",
    "            attempts = 0\n",
    "            max_attempts = 20\n",
    "            negative_found = False\n",
    "            while attempts < max_attempts:\n",
    "                num_to_shuffle = random.randint(2, len(utts))\n",
    "                selected = random.sample(range(len(utts)), num_to_shuffle)\n",
    "                shuffled_utts = utts.copy()\n",
    "                utts_to_shuffle = [shuffled_utts[i] for i in selected]\n",
    "                random.shuffle(utts_to_shuffle)\n",
    "                for idx, pos in enumerate(selected):\n",
    "                    shuffled_utts[pos] = utts_to_shuffle[idx]\n",
    "                if shuffled_utts != utts:\n",
    "                    negative_found = True\n",
    "                    break\n",
    "                attempts += 1\n",
    "\n",
    "            if not negative_found:\n",
    "                continue\n",
    "\n",
    "            neg_dialogue = original_dialogue.copy()\n",
    "            for pos, orig_idx in enumerate(indices):\n",
    "                neg_dialogue[orig_idx] = (target_spk, shuffled_utts[pos])\n",
    "\n",
    "            if neg_dialogue != original_dialogue:\n",
    "                self.dialogues.append(neg_dialogue)\n",
    "                self.labels.append(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        utterances = [utt for _, utt in dialogue]\n",
    "        return utterances, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCTH5_0oJwxM"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"SI2M-Lab/DarijaBERT\", n_unfrozen=2):\n",
    "        super().__init__()\n",
    "        # Load the tokenizer and model from HuggingFace\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze all model parameters by default\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the last n_unfrozen encoder layers for fine-tuning\n",
    "        n_layers = len(self.model.encoder.layer)\n",
    "        for i in range(n_layers - n_unfrozen, n_layers):\n",
    "            for param in self.model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, utterances):\n",
    "        batch = self.tokenizer(\n",
    "            utterances,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch = {k: v.to(self.model.device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get model outputs (last_hidden_state)\n",
    "        outputs = self.model(**batch)\n",
    "        \n",
    "        # Return the embedding of the [CLS] token for each utterance\n",
    "        return outputs.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctK6gVZJEXi2"
   },
   "outputs": [],
   "source": [
    "class OrderAwareDocEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, nhead=4, num_layers=2, dropout=0.1, max_len=32):\n",
    "        super().__init__()\n",
    "        # Learnable positional embeddings for encoding order information\n",
    "        self.positional = nn.Parameter(torch.zeros(1, max_len, hidden_size))\n",
    "        nn.init.normal_(self.positional, std=0.02)  # Initialize with normal distribution\n",
    "\n",
    "        # Transformer encoder layer for modeling sentence order and context\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,    # Embedding dimension\n",
    "            nhead=nhead,            # Number of attention heads\n",
    "            dropout=dropout,        # Dropout rate\n",
    "            batch_first=True        # (batch, seq, features) format\n",
    "        )\n",
    "        # Stack multiple Transformer encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Dense (fully connected) layers for classification\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, 1)  # Final output layer for logits (single value per document)\n",
    "\n",
    "    def forward(self, sent_emb, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sent_emb: Tensor of shape (batch_size, seq_len, hidden_size), sentence embeddings for each document\n",
    "            mask: Tensor of shape (batch_size, seq_len), 1 for real sentences, 0 for padding\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, 1), document coherence logits\n",
    "        \"\"\"\n",
    "        seq_len = sent_emb.size(1)\n",
    "\n",
    "        # Slice positional embedding to match input sequence length, and move to input device\n",
    "        pos_emb = self.positional[:, :seq_len, :].to(sent_emb.device)\n",
    "\n",
    "        # Add positional information to sentence embeddings\n",
    "        x = sent_emb + pos_emb\n",
    "\n",
    "        # Build the mask for the Transformer: True for PAD positions (to be ignored)\n",
    "        transformer_mask = ~mask.bool()\n",
    "        # Pass through the Transformer encoder\n",
    "        x = self.encoder(x, src_key_padding_mask=transformer_mask)\n",
    "\n",
    "        # Use the embedding of the first token ([CLS]) as document representation\n",
    "        doc_emb = x[:, 0, :]\n",
    "\n",
    "        # Pass through dense layers and activation for classification\n",
    "        h = self.dense(doc_emb)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        logits = self.out(h)  # Output logits (before sigmoid or softmax)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs74RQAbnlTM"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    utter_lists, labels = zip(*batch)\n",
    "    maxlen = max(len(utter) for utter in utter_lists)\n",
    "    padded_utts = [utts + [\"\"] * (maxlen - len(utts)) for utts in utter_lists]\n",
    "    mask = torch.tensor([[1]*len(utts) + [0]*(maxlen-len(utts)) for utts in utter_lists], dtype=torch.float)\n",
    "    labels = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "    # Return the padded utterance lists, mask, and labels\n",
    "    return padded_utts, mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yvx_5FhHnnpA"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(sent_encoder, doc_encoder, dataloader, optimizer, device, criterion):\n",
    "    # Set both encoders to training mode (enables dropout, gradient updates, etc.)\n",
    "    sent_encoder.train()\n",
    "    doc_encoder.train()\n",
    "    total_loss, total_correct, total = 0, 0, 0  # Track total loss and accuracy\n",
    "\n",
    "    # Iterate over each batch in the dataloader\n",
    "    for utter_lists, mask, labels in tqdm(dataloader):\n",
    "        batch_size = len(utter_lists)  # Number of samples in the batch\n",
    "\n",
    "        # Flatten all utterances in the batch into a single list for encoding\n",
    "        flat_utterances = [utt for dialogue in utter_lists for utt in dialogue]\n",
    "\n",
    "        # Get embeddings for every utterance in the batch\n",
    "        flat_embeddings = sent_encoder(flat_utterances)\n",
    "\n",
    "        # Figure out how to split flat_embeddings back into dialogues\n",
    "        splits = [len(utts) for utts in utter_lists]\n",
    "        utter_emb_batch = torch.split(flat_embeddings, splits, dim=0)\n",
    "\n",
    "        # Pad all dialogue embeddings to the same sequence length (maxlen)\n",
    "        maxlen = max(splits)\n",
    "        utter_emb_batch = [\n",
    "            torch.cat([e, torch.zeros(maxlen - e.size(0), e.size(1), device=device)], dim=0)\n",
    "            for e in utter_emb_batch\n",
    "        ]\n",
    "        # Stack into a 3D tensor (batch_size, seq_len, hidden_size)\n",
    "        utter_emb_batch = torch.stack(utter_emb_batch)\n",
    "\n",
    "        # Move mask and labels to the same device (CPU/GPU)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass: get logits from document encoder\n",
    "        logits = doc_encoder(utter_emb_batch, mask)\n",
    "        loss = criterion(logits, labels)  # Compute loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute predictions and update accuracy statistics\n",
    "        preds = (torch.sigmoid(logits) > 0.5).long()  # Convert logits to binary predictions\n",
    "        total_correct += (preds == labels.long()).sum().item()\n",
    "        total += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "    # Calculate average loss and accuracy over the epoch\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = total_correct / total\n",
    "    print(f\"Train loss: {avg_loss:.4f} | Train acc: {avg_acc:.4f}\")\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoXjjCzsNivy"
   },
   "outputs": [],
   "source": [
    "#Same as the train function, however without computing gradients\n",
    "def val_one_epoch(sent_encoder, doc_encoder, dataloader, device, criterion):\n",
    "    sent_encoder.eval()\n",
    "    doc_encoder.eval()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for utter_lists, mask, labels in tqdm(dataloader):\n",
    "            batch_size = len(utter_lists)\n",
    "            flat_utterances = [utt for dialogue in utter_lists for utt in dialogue]\n",
    "            flat_embeddings = sent_encoder(flat_utterances)\n",
    "            splits = [len(utts) for utts in utter_lists]\n",
    "            utter_emb_batch = torch.split(flat_embeddings, splits, dim=0)\n",
    "            maxlen = max(splits)\n",
    "            utter_emb_batch = [\n",
    "                torch.cat([e, torch.zeros(maxlen - e.size(0), e.size(1), device=device)], dim=0)\n",
    "                for e in utter_emb_batch\n",
    "            ]\n",
    "            utter_emb_batch = torch.stack(utter_emb_batch)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = doc_encoder(utter_emb_batch, mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy().flatten()\n",
    "            true_labels = labels.long().cpu().numpy().flatten()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(true_labels.tolist())\n",
    "\n",
    "            total_correct += (preds == true_labels).sum()\n",
    "            total += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = total_correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "    print(f\"Val loss: {avg_loss:.4f} | Val acc: {avg_acc:.4f} \")\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_Qe3YDfChTl"
   },
   "outputs": [],
   "source": [
    "#Same as the validation function  \n",
    "def test_one_epoch(sent_encoder, doc_encoder, dataloader, device, criterion):\n",
    "    sent_encoder.eval()\n",
    "    doc_encoder.eval()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for utter_lists, mask, labels in tqdm(dataloader):\n",
    "            batch_size = len(utter_lists)\n",
    "            flat_utterances = [utt for dialogue in utter_lists for utt in dialogue]\n",
    "            flat_embeddings = sent_encoder(flat_utterances)\n",
    "            splits = [len(utts) for utts in utter_lists]\n",
    "            utter_emb_batch = torch.split(flat_embeddings, splits, dim=0)\n",
    "            maxlen = max(splits)\n",
    "            utter_emb_batch = [\n",
    "                torch.cat([e, torch.zeros(maxlen - e.size(0), e.size(1), device=device)], dim=0)\n",
    "                for e in utter_emb_batch\n",
    "            ]\n",
    "            utter_emb_batch = torch.stack(utter_emb_batch)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = doc_encoder(utter_emb_batch, mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy().flatten()\n",
    "            true_labels = labels.long().cpu().numpy().flatten()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(true_labels.tolist())\n",
    "\n",
    "            total_correct += (preds == true_labels).sum()\n",
    "            total += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = total_correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "    print(f\"Test loss: {avg_loss:.4f} | Test acc: {avg_acc:.4f} | Test F1: {f1:.4f}\")\n",
    "    return avg_loss, avg_acc, f1, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pEPhGlhnpX3",
    "outputId": "7072fa63-0bb1-4691-e595-cc7e744dafe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:30<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6924 | Train acc: 0.5299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6594 | Val acc: 0.6583 \n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:27<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6648 | Train acc: 0.5953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6293 | Val acc: 0.6457 \n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:29<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6472 | Train acc: 0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6115 | Val acc: 0.6482 \n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:27<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6206 | Train acc: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6084 | Val acc: 0.6709 \n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:28<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6026 | Train acc: 0.6508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5870 | Val acc: 0.6734 \n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:27<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5959 | Train acc: 0.6541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5883 | Val acc: 0.6784 \n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:28<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5737 | Train acc: 0.6630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6217 | Val acc: 0.5402 \n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:27<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5672 | Train acc: 0.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5731 | Val acc: 0.6759 \n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:27<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5527 | Train acc: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5644 | Val acc: 0.6784 \n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:28<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5506 | Train acc: 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5688 | Val acc: 0.6734 \n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:28<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5496 | Train acc: 0.6763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5632 | Val acc: 0.6759 \n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:28<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5461 | Train acc: 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5667 | Val acc: 0.6759 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training cell (Skip if using pre-trained models)\n",
    "\n",
    "# Settings \n",
    "\n",
    "TRAIN_DATA_FOLDER = \"final_datasets_train\"\n",
    "VAL_DATA_FOLDER = \"final_datasets_test\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 12\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare dataset and loader\n",
    "train_dataset = DialogueCoherenceDataset(TRAIN_DATA_FOLDER)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = DialogueCoherenceDataset(VAL_DATA_FOLDER)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "n_positive = sum(train_dataset.labels)\n",
    "n_negative = len(train_dataset.labels) - n_positive\n",
    "pos_weight = torch.tensor([n_negative / n_positive]).to(DEVICE)\n",
    "train_criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "val_n_positive = sum(val_dataset.labels)\n",
    "val_n_negative = len(val_dataset.labels) - val_n_positive\n",
    "val_pos_weight = torch.tensor([val_n_negative / val_n_positive]).to(DEVICE)\n",
    "val_criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "\n",
    "# Build models\n",
    "sent_encoder = SentenceEncoder(n_unfrozen=2).to(DEVICE)\n",
    "doc_encoder = OrderAwareDocEncoder(hidden_size=768, num_layers=2, nhead=4, dropout=0.1, max_len=32).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(doc_encoder.parameters()) +\n",
    "    list(filter(lambda p: p.requires_grad, sent_encoder.parameters())),\n",
    "    lr=1e-5\n",
    ")\n",
    "\n",
    "best_val_acc=0.0\n",
    "\n",
    "# Training loop, saving the model whenever we reach a higher validation accuracy to avoid overfitting \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_one_epoch(sent_encoder, doc_encoder, train_dataloader, optimizer, DEVICE, train_criterion)\n",
    "    val_loss, val_acc=val_one_epoch(sent_encoder, doc_encoder, val_dataloader, DEVICE, val_criterion)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc=val_acc\n",
    "        torch.save(\n",
    "                {\n",
    "                    \"sent_encoder_state_dict\": sent_encoder.state_dict(),\n",
    "                    \"doc_encoder_state_dict\": doc_encoder.state_dict()\n",
    "                }, \"best_HT_in\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULcuOWD-Cq3q",
    "outputId": "a4eb1dee-59e8-4f3c-9795-3bef39ef2630"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5667 | Test acc: 0.6759 | Test F1: 0.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"final_datasets_test\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Prepare dataset and loader\n",
    "test_dataset = DialogueCoherenceDataset(DATA_FOLDER)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_n_positive = sum(test_dataset.labels)\n",
    "test_n_negative = len(test_dataset.labels) - test_n_positive\n",
    "test_pos_weight = torch.tensor([test_n_negative / test_n_positive]).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=test_pos_weight)\n",
    "\n",
    "# Load pre-trained model from checkpoint \n",
    "\n",
    "checkpoint = torch.load(\"best_HT_in\", map_location='cpu')  # or 'cuda' if on GPU\n",
    "\n",
    "\n",
    "\n",
    "sent_encoder = SentenceEncoder(n_unfrozen=2).to(DEVICE)\n",
    "sent_encoder.load_state_dict(checkpoint[\"sent_encoder_state_dict\"])\n",
    "\n",
    "doc_encoder = OrderAwareDocEncoder(hidden_size=768, num_layers=2, nhead=4, dropout=0.1, max_len=32).to(DEVICE)\n",
    "doc_encoder.load_state_dict(checkpoint[\"doc_encoder_state_dict\"])\n",
    "\n",
    "\n",
    "\n",
    "# Run testing\n",
    "\n",
    "val_loss, val_acc, val_f1, val_preds, val_labels = test_one_epoch(\n",
    "    sent_encoder,\n",
    "    doc_encoder,\n",
    "    test_dataloader,\n",
    "    DEVICE,\n",
    "    criterion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4r9oXtUEqg5",
    "outputId": "e929898b-f4d6-43dc-9b19-56ccaa8bb3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 86 113]\n",
      " [ 16 183]]\n"
     ]
    }
   ],
   "source": [
    "#Printing the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
